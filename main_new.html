import os
import json
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from langchain_openai import AzureChatOpenAI

# Initialize FastAPI app
app = FastAPI()

# Initialize LLM
llm = AzureChatOpenAI(
    azure_deployment="gpt-35-turbo",  # or your deployment
    api_version="2023-06-01-preview",  # or your api version
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

# To hold the uploaded OpenAPI data
openapi_data = {}

class QueryRequest(BaseModel):
    user_input: str


@app.post("/upload_openapi_url")
async def upload_openapi_url(url: str):
    try:
        response = await fetch_openapi_data(url)
        openapi_data["url"] = response
        return {"detail": "OpenAPI loaded successfully."}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.post("/submit_openapi")
async def submit_openapi(query: QueryRequest):
    user_input = query.user_input

    if not openapi_data:
        raise HTTPException(status_code=400, detail="OpenAPI data not uploaded yet.")

    # Query the LLM for a response based on the user input
    response = await llm.call(user_input)
    
    return JSONResponse(content={"response": response})
    

# Helper function to simulate fetching OpenAPI data
async def fetch_openapi_data(url: str):
    # Simulate fetching data from a URL
    return {"url": url, "data": "Sample OpenAPI data loaded from URL"}
